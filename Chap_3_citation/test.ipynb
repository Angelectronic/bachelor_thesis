{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[34, 34], edge_index=[2, 112], y=[34], train_mask=[34], pos_edge_label=[56], pos_edge_label_index=[2, 56], neg_edge_label=[56], neg_edge_label_index=[2, 56])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.datasets import KarateClub\n",
    "import torch\n",
    "from torch_geometric.nn import GAE, GCNConv\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataset = KarateClub()\n",
    "data = dataset[0].to(device)\n",
    "transform = RandomLinkSplit(is_undirected=True, split_labels=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([34, 16])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu_()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCNEncoder(hidden_channels=16).to(device)\n",
    "output = model(data.x, data.edge_index)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistMultDecoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z, edge_index, sigmoid: bool = True):\n",
    "        z_src, z_dst = z[edge_index[0].long()], z[edge_index[1].long()]\n",
    "        z_x = z_src * z_dst\n",
    "        out = torch.sum(z_x, dim=1)\n",
    "        return torch.sigmoid(out) if sigmoid else out\n",
    "    \n",
    "model = GAE(GCNEncoder(hidden_channels=16), DistMultDecoder()).to(device)\n",
    "model.reset_parameters()\n",
    "model(data.x, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    train_loss = model.recon_loss(z, train_data.pos_edge_label_index, train_data.neg_edge_label_index)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    return train_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    z = model.encode(test_data.x, test_data.edge_index)\n",
    "    train_acc = model.test(z, test_data.pos_edge_label_index, test_data.neg_edge_label_index)\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.3509\n",
      "Test Acc: 0.6978, Test AUC: 0.7034\n",
      "Epoch: 02, Loss: 1.3379\n",
      "Test Acc: 0.6311, Test AUC: 0.6578\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train_loss = train()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {train_loss:.4f}')\n",
    "\n",
    "    test_acc = test()\n",
    "    print(f'Test Acc: {test_acc[0]:.4f}, Test AUC: {test_acc[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def load_json(filename):\n",
    "    \"\"\"Đọc dữ liệu từ file JSON.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def calc_sentence_sim(s1, s2):\n",
    "    \"\"\"Tính độ tương đồng giữa hai câu dựa trên từ chung.\"\"\"\n",
    "    s1 = s1.split()\n",
    "    s2 = s2.split()\n",
    "    if len(s1) == 0 or len(s2) == 0:\n",
    "        return 0  # Tránh chia cho 0\n",
    "    return len(set(s1) & set(s2)) / (np.log(len(s1) + 1) + np.log(len(s2) + 1))\n",
    "\n",
    "def prepare_data(data_dir):\n",
    "    graphs = []\n",
    "    labels = []\n",
    "    \n",
    "    in_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.in')])\n",
    "    total_files = len(in_files)\n",
    "    \n",
    "    for idx, in_file in enumerate(in_files):\n",
    "        print(f\"Processing file {idx+1}/{total_files}\")  # In ra tiến độ xử lý\n",
    "        \n",
    "        current_positive_samples = 0\n",
    "        current_negative_samples = 0\n",
    "        \n",
    "        base_name = in_file.replace('.in', '')\n",
    "        label_file = os.path.join(data_dir, f\"{base_name}.label\")\n",
    "        \n",
    "        if not os.path.exists(label_file):\n",
    "            print(f\"Warning: Missing label file for {in_file}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        in_data = load_json(os.path.join(data_dir, in_file))\n",
    "        label_data = load_json(label_file)\n",
    "        \n",
    "        correct_citations = set(label_data[\"correct_citation\"])\n",
    "        citation_ids = in_data[\"citation_candidates\"]\n",
    "        \n",
    "        try:\n",
    "            embedding_sentences = load_json(f'./specter_embeddings_task1/{base_name}.json')[\"sentences\"]\n",
    "            text_sentences = load_json(f'./tachcautask1/tachcautask1/{base_name}.sen')[\"sentences\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error loading embeddings or text sentences for {base_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        for cid in citation_ids:\n",
    "            if current_positive_samples == len(correct_citations) and current_negative_samples >= 3:\n",
    "                break\n",
    "            \n",
    "            label = 1 if cid in correct_citations else 0\n",
    "            labels.append(label)\n",
    "            if label == 1:\n",
    "                current_positive_samples += 1\n",
    "            else:\n",
    "                current_negative_samples += 1\n",
    "            \n",
    "            try:\n",
    "                candidate_vec = load_json(f\"./candidates_storage_vec/{cid}.candidate\")\n",
    "                candidate_text = load_json(f\"./candidates_storage/{cid}.candidate\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Missing file {cid}.candidate, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            title_embedding = np.array(candidate_vec['title'])\n",
    "            abstract_embeddings = np.array(candidate_vec['abstract'])\n",
    "            \n",
    "            abstract_texts = candidate_text['abstract']\n",
    "            title_text = candidate_text['title']\n",
    "            \n",
    "            nodes = np.vstack([embedding_sentences, title_embedding, abstract_embeddings])\n",
    "            num_nodes = nodes.shape[0]\n",
    "            \n",
    "            similarity_matrix = cosine_similarity(nodes)\n",
    "            \n",
    "            edges = []\n",
    "            edge_weights = []\n",
    "            \n",
    "            num_source_sentences = len(embedding_sentences)\n",
    "            num_abstract_sentences = len(abstract_embeddings)\n",
    "            title_idx = num_source_sentences\n",
    "            offset = num_source_sentences + 1\n",
    "            \n",
    "            for i in range(num_nodes):\n",
    "                for j in range(num_nodes):\n",
    "                    if i != j:\n",
    "                        edges.append([i, j])\n",
    "                        \n",
    "                        window_flag = 0\n",
    "                        if (0 <= i < num_source_sentences and j == i + 1) or \\\n",
    "                           (offset <= i < offset + num_abstract_sentences and j == i + 1):\n",
    "                            window_flag = 1\n",
    "                        \n",
    "                        if i < num_source_sentences:\n",
    "                            text_i = text_sentences[i] if i < len(text_sentences) else \"\"\n",
    "                        elif i == title_idx:\n",
    "                            text_i = title_text\n",
    "                        else:\n",
    "                            text_i = abstract_texts[i - offset]\n",
    "                        \n",
    "                        if j < num_source_sentences:\n",
    "                            text_j = text_sentences[j]\n",
    "                        elif j == title_idx:\n",
    "                            text_j = title_text\n",
    "                        else:\n",
    "                            text_j = abstract_texts[j - offset]\n",
    "                        \n",
    "                        text_sim = calc_sentence_sim(text_i, text_j)\n",
    "                        \n",
    "                        weight = [\n",
    "                            similarity_matrix[i, j],\n",
    "                            text_sim,\n",
    "                            window_flag\n",
    "                        ]\n",
    "                        edge_weights.append(weight)\n",
    "            \n",
    "            graph_dict = {\n",
    "                \"nodes\": nodes.astype(np.float32),\n",
    "                \"edges\": np.array(edges, dtype=np.int64).T,\n",
    "                \"weights\": np.array(edge_weights, dtype=np.float32)\n",
    "            }\n",
    "            graphs.append(graph_dict)\n",
    "    \n",
    "    return graphs, labels\n",
    "\n",
    "def convert_to_pyg_format(graphs, labels):\n",
    "    \"\"\"Chuyển đổi danh sách graphs sang định dạng của PyTorch Geometric.\"\"\"\n",
    "    data_list = []\n",
    "    for graph, label in zip(graphs, labels):\n",
    "        x = torch.tensor(graph['nodes'], dtype=torch.float)          # Node features\n",
    "        edge_index = torch.tensor(graph['edges'], dtype=torch.long)    # Edge connectivity\n",
    "        edge_attr = torch.tensor(graph['weights'], dtype=torch.float)  # Edge weights\n",
    "        \n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=torch.tensor([label], dtype=torch.long)\n",
    "        )\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "def process_and_save_data(data_dir, processed_file):\n",
    "    \"\"\"Xử lý dữ liệu thô và lưu dataset đã xử lý (dưới dạng list of dictionaries) vào file.\"\"\"\n",
    "    graphs, labels = prepare_data(data_dir)\n",
    "    dataset = convert_to_pyg_format(graphs, labels)\n",
    "    \n",
    "    # Chuyển đổi các Data object thành dictionary để tránh lỗi unpickle\n",
    "    dataset_dict = []\n",
    "    for data in dataset:\n",
    "        dataset_dict.append({\n",
    "            \"x\": data.x,\n",
    "            \"edge_index\": data.edge_index,\n",
    "            \"edge_attr\": data.edge_attr,\n",
    "            \"y\": data.y,\n",
    "        })\n",
    "    \n",
    "    torch.save(dataset_dict, processed_file)\n",
    "    print(f\"Processed dataset saved to {processed_file}\")\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dataset = [{\n",
    "    'x': torch.randn(3, 768),\n",
    "    'edge_index': torch.tensor([[0, 1, 2], [1, 2, 0]], dtype=torch.long),\n",
    "    'edge_attr': torch.randn(3, 3),\n",
    "    'y': torch.tensor([0])\n",
    "}, {\n",
    "    'x': torch.randn(4, 768),\n",
    "    'edge_index': torch.tensor([[0, 1, 1, 2], [1, 0, 2, 3]], dtype=torch.long),\n",
    "    'edge_attr': torch.randn(4, 3),\n",
    "    'y': torch.tensor([1])\n",
    "}]\n",
    "\n",
    "for data in dataset:\n",
    "    data_json = {\n",
    "        \"x\": data['x'].tolist(),\n",
    "        \"edge_index\": data['edge_index'].tolist(),\n",
    "        \"edge_attr\": data['edge_attr'].tolist(),\n",
    "        \"y\": data['y'].tolist(),\n",
    "    }\n",
    "\n",
    "    with open(f\"data.json\", 'a') as f:\n",
    "        f.write(json.dumps(data_json) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class MyOwnDataset(Dataset):\n",
    "    def __init__(self, root=None, transform=None, pre_transform=None, pre_filter=None, dataset=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        return Data(\n",
    "            x=torch.tensor(data['x'], dtype=torch.float),\n",
    "            edge_index=torch.tensor(data['edge_index'], dtype=torch.long),\n",
    "            edge_attr=torch.tensor(data['edge_attr'], dtype=torch.float),\n",
    "            y=torch.tensor(data['y'], dtype=torch.long)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['x', 'edge_index', 'edge_attr', 'y'],\n",
      "    num_rows: 1\n",
      "})\n",
      "DataBatch(x=[3, 768], edge_index=[2, 3], edge_attr=[3, 3], y=[1], batch=[3], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"data.json\", split=\"train\")\n",
    "dataset = dataset.train_test_split(train_size=0.8)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "graph_dataset = MyOwnDataset(dataset=train_dataset)\n",
    "test_data = MyOwnDataset(dataset=test_dataset)\n",
    "loader = DataLoader(graph_dataset, batch_size=2, shuffle=True)\n",
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
