{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-09T16:39:59.149259Z",
     "iopub.status.busy": "2024-07-09T16:39:59.148575Z",
     "iopub.status.idle": "2024-07-09T16:43:48.851073Z",
     "shell.execute_reply": "2024-07-09T16:43:48.849824Z",
     "shell.execute_reply.started": "2024-07-09T16:39:59.149228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n",
    "!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T16:43:48.853262Z",
     "iopub.status.busy": "2024-07-09T16:43:48.852983Z",
     "iopub.status.idle": "2024-07-09T16:44:07.687956Z",
     "shell.execute_reply": "2024-07-09T16:44:07.686819Z",
     "shell.execute_reply.started": "2024-07-09T16:43:48.853235Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 16:43:58.055311: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-09 16:43:58.055467: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-09 16:43:58.192615: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T16:44:48.591067Z",
     "iopub.status.busy": "2024-07-09T16:44:48.590340Z",
     "iopub.status.idle": "2024-07-09T16:44:48.811958Z",
     "shell.execute_reply": "2024-07-09T16:44:48.811040Z",
     "shell.execute_reply.started": "2024-07-09T16:44:48.591036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T16:44:50.784265Z",
     "iopub.status.busy": "2024-07-09T16:44:50.783593Z",
     "iopub.status.idle": "2024-07-09T16:45:00.713893Z",
     "shell.execute_reply": "2024-07-09T16:45:00.713017Z",
     "shell.execute_reply.started": "2024-07-09T16:44:50.784232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a23a8d38ec446e584354f33491377f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1998d357e22d493cbdfdca4ed240f6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/511k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d026dcbb28a3466ab878441795f66c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/156k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55dcc67a26ff4635a97fda680997a49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/94.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65c8d64b8f44052aac6298979738316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cecf6aafb44d21a93c1148cde2a12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cc01b7c8f74e7c9a1ea2a93323c019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'options', 'answers'],\n",
       "        num_rows: 1936\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['context', 'question', 'options', 'answers'],\n",
       "        num_rows: 551\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['context', 'question', 'options', 'answers'],\n",
       "        num_rows: 296\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"Angelectronic/ViMMRC\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T16:45:19.112435Z",
     "iopub.status.busy": "2024-07-09T16:45:19.111781Z",
     "iopub.status.idle": "2024-07-09T16:48:47.737216Z",
     "shell.execute_reply": "2024-07-09T16:48:47.736148Z",
     "shell.execute_reply.started": "2024-07-09T16:45:19.112390Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1033174351e64d60986bf406efcfa605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4320b39786224f30820f2c59c457c6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216a2e72fb63435e8f8f111d70a155a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7beae5d4844c2f863336b5033e7759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5fc35ae640421991abe99f80882321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2983cf10084ba9a6410d1b120f8b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = '',\n",
    ")\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T16:50:33.141312Z",
     "iopub.status.busy": "2024-07-09T16:50:33.140448Z",
     "iopub.status.idle": "2024-07-09T16:50:33.869990Z",
     "shell.execute_reply": "2024-07-09T16:50:33.869072Z",
     "shell.execute_reply.started": "2024-07-09T16:50:33.141278Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbd74174da64f5f9789cd336d91db72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be434f6b5c04be296e8b6841eb0d52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Use the following reference to answer the given multiple choice-question:\n",
      "Reference: Kh√°ch ƒë·∫øn x√£ Tr·ªãnh T∆∞·ªùng, huy·ªán B√°t X√°t, t·ªânh L√†o Cai  s·∫Ω kh√¥ng kh·ªèi ng·ª° ng√†ng th·∫•y m·ªôt d√≤ng m∆∞∆°ng ngo·∫±n ngo√®o v·∫Øt ngang nh·ªØng ƒë·ªìi cao. D√¢n b·∫£n g·ªçi d√≤ng m∆∞∆°ng ·∫•y l√† con n∆∞·ªõc √¥ng L√¨n. ƒê·ªÉ thay ƒë·ªïi t·∫≠p qu√°n l√†m l√∫a n∆∞∆°ng, √¥ng Ph√†n Ph√π L√¨n, ng∆∞·ªùi Dao ·ªü th√¥n Ph√¨n Ngan ƒë√£ l·∫ßn m√≤ c·∫£ th√°ng trong r·ª´ng t√¨m ngu·ªìn n∆∞·ªõc. Nh∆∞ng t√¨m ƒë∆∞·ª£c ngu·ªìn n∆∞·ªõc r·ªìi, m·ªçi ng∆∞·ªùi v·∫´n kh√¥ng tin c√≥ th·ªÉ d·∫´n n∆∞·ªõc v·ªÅ. √îng c√πng v·ª£ con ƒë√†o su·ªët m·ªôt nƒÉm tr·ªùi ƒë∆∞·ª£c g·∫ßn b·ªën c√¢y s·ªë m∆∞∆°ng xuy√™n ƒë·ªìi d·∫´n n∆∞·ªõc t·ª´ r·ª´ng gi√† v·ªÅ th√¥n, tr·ªìng m·ªôt h√©c ta l√∫a n∆∞·ªõc ƒë·ªÉ b√† con tin. R·ªìi √¥ng v·∫≠n ƒë·ªông m·ªçi ng∆∞·ªùi c√πng m·ªü r·ªông con m∆∞∆°ng, v·ª° th√™m ƒë·∫•t hoang tr·ªìng l√∫a.\n",
      "\n",
      "Con n∆∞·ªõc nh·ªè ƒë√£ l√†m thay ƒë·ªïi t·∫≠p qu√°n canh t√°c v√† cu·ªôc s·ªëng c·ªßa tr√™n 50 h·ªô trong th√¥n. Nh·ªØng n∆∞∆°ng l√∫a quanh nƒÉm kh√°t n∆∞·ªõc ƒë∆∞·ª£c thay d·∫ßn b·∫±ng ru·ªông b·∫≠c thang. Nh·ªØng gi·ªëng l√∫a lai cao s·∫£n ƒë∆∞·ª£c √¥ng L√¨n ƒë∆∞a v·ªÅ v·∫≠n ƒë·ªông b√† con tr·ªìng c·∫•y, nh·ªù v·∫≠y m√† c·∫£ th√¥n kh√¥ng c√≤n h·ªô ƒë√≥i. T·ª´ khi n∆∞·ªõc ƒë∆∞·ª£c d·∫´n v·ªÅ th√¥n, nh√† ai c≈©ng c·∫•y l√∫a n∆∞·ªõc ch·ª© kh√¥ng ph√° r·ª´ng l√†m n∆∞∆°ng nh∆∞ tr∆∞·ªõc n·ªØa.\n",
      "\n",
      "Mu·ªën c√≥ n∆∞·ªõc c·∫•y l√∫a th√¨ ph·∫£i gi·ªØ r·ª´ng. √îng L√¨n l·∫∑n l·ªôi ƒë·∫øn c√°c x√£ b·∫°n h·ªçc c√°ch tr·ªìng c√¢y th·∫£o qu·∫£ v·ªÅ h∆∞·ªõng d·∫´n cho b√† con c√πng l√†m. Nhi·ªÅu h·ªô trong th√¥n m·ªói nƒÉm thu ƒë∆∞·ª£c m·∫•y ch·ª•c tri·ªáu ƒë·ªìng t·ª´ lo·∫°i c√¢y n√†y. Ri√™ng gia ƒë√¨nh √¥ng L√¨n m·ªói nƒÉm thu hai trƒÉm tri·ªáu. Ph√¨n Ngan t·ª´ th√¥n ngh√®o nh·∫•t ƒë√£ v∆∞∆°n l√™n th√†nh th√¥n c√≥ m·ª©c s·ªëng kh√° nh·∫•t c·ªßa x√£ Tr·ªãnh T∆∞·ªùng.\n",
      "\n",
      "Chuy·ªán c·ªßa Ngu C√¥ng x√£ Tr·ªãnh T∆∞·ªùng nhanh ch√≥ng bay v·ªÅ Th·ªß ƒë√¥. √îng Ph√†n Ph√π Lin vinh d·ª± ƒë∆∞·ª£c Ch·ªß t·ªãch n∆∞·ªõc g·ª≠i th∆∞ khen ng·ª£i.\n",
      "Question: Gia ƒë√¨nh √¥ng L√¨n thu ƒë∆∞·ª£c k·∫øt qu·∫£ nh∆∞ th·∫ø n√†o t·ª´ vi·ªác tr·ªìng c√¢y th·∫£o qu·∫£?\n",
      "A. M·∫•y ch·ª•c tri·ªáu.\n",
      "B. M·ªói nƒÉm thu hai trƒÉm tri·ªáu.\n",
      "C. M·ªói nƒÉm thu hai ch·ª•c tri·ªáu.\n",
      "D. M·ªói nƒÉm thu hai t·ª∑.<|eot_id|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "B<|eot_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "END_TURN_TOKEN = '<end_of_turn>'\n",
    "BOS_TOKEN = tokenizer.bos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for i in range(len(examples['question'])):\n",
    "        option_prompt = \"\"\n",
    "        for j in range(len(examples['options'][i])):\n",
    "            if j == 0:\n",
    "                option_prompt += \"A. \" + examples['options'][i][j] + \"\\n\"\n",
    "            elif j == 1:\n",
    "                option_prompt += \"B. \" + examples['options'][i][j] + \"\\n\"\n",
    "            elif j == 2:\n",
    "                option_prompt += \"C. \" + examples['options'][i][j] + \"\\n\"\n",
    "            elif j == 3:\n",
    "                option_prompt += \"D. \" + examples['options'][i][j] + \"\\n\"\n",
    "\n",
    "        chat = [\n",
    "            {\"role\": \"user\", \"content\": \"Use the following reference to answer the given multiple choice-question:\\nReference: \" + examples['context'][i] + \"\\nQuestion: \" + examples['question'][i] + \"\\n\" + option_prompt},\n",
    "            {\"role\": \"system\", \"content\": examples['answers'][i]}\n",
    "        ]\n",
    "        texts.append(tokenizer.apply_chat_template(chat, tokenize=False) + EOS_TOKEN)\n",
    "\n",
    "    texts = [text.split(BOS_TOKEN)[-1] for text in texts]\n",
    "    return {\"text\": texts}\n",
    "pass\n",
    "dataset['train'] = dataset['train'].map(formatting_prompts_func, batched=True)\n",
    "dataset['test'] = dataset['test'].map(formatting_prompts_func, batched=True)\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=\"<start_of_turn>user\", response_template=\"<start_of_turn>model\", tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "print(dataset['train'][-1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "END_TURN_TOKEN = '<|eot_id|>'\n",
    "BOS_TOKEN = tokenizer.bos_token\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "labels = [dataset['test'][i]['answers'] for i in range(len(dataset['test']))]\n",
    "preds = []\n",
    "preds1 = []\n",
    "FastLanguageModel.for_inference(model)\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    option_prompt = \"\"\n",
    "    for j in range(len(dataset['test'][i]['options'])):\n",
    "        if j == 0:\n",
    "            option_prompt += \"A. \" + dataset['test'][i]['options'][j] + \"\\n\"\n",
    "        elif j == 1:\n",
    "            option_prompt += \"B. \" + dataset['test'][i]['options'][j] + \"\\n\"\n",
    "        elif j == 2:\n",
    "            option_prompt += \"C. \" + dataset['test'][i]['options'][j] + \"\\n\"\n",
    "        elif j == 3:\n",
    "            option_prompt += \"D. \" + dataset['test'][i]['options'][j] + \"\\n\"\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Use the following reference to answer the given multiple choice-question, respond only with the letter before the correct answer (A, B, C or D) and do not add anything else :\\nReference: \" + dataset['test'][i]['context'] + \"\\nQuestion: \" + dataset['test'][i]['question'] + \"\\n\" + option_prompt},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    prompt = prompt.split(BOS_TOKEN)[-1]\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256, use_cache = True, do_sample = True, eos_token_id=model.config.eos_token_id, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#     logits = outputs.logits\n",
    "#     token_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    completion = prediction.split('<|start_header_id|>assistant<|end_header_id|>')[-1].split(END_TURN_TOKEN)[0].strip()\n",
    "    preds.append(completion)\n",
    "\n",
    "print(accuracy_score(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
