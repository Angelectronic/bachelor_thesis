{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-09T16:39:59.149259Z",
     "iopub.status.busy": "2024-07-09T16:39:59.148575Z",
     "iopub.status.idle": "2024-07-09T16:43:48.851073Z",
     "shell.execute_reply": "2024-07-09T16:43:48.849824Z",
     "shell.execute_reply.started": "2024-07-09T16:39:59.149228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n",
    "!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T16:43:48.853262Z",
     "iopub.status.busy": "2024-07-09T16:43:48.852983Z",
     "iopub.status.idle": "2024-07-09T16:44:07.687956Z",
     "shell.execute_reply": "2024-07-09T16:44:07.686819Z",
     "shell.execute_reply.started": "2024-07-09T16:43:48.853235Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 16:43:58.055311: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-09 16:43:58.055467: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-09 16:43:58.192615: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T16:44:48.591067Z",
     "iopub.status.busy": "2024-07-09T16:44:48.590340Z",
     "iopub.status.idle": "2024-07-09T16:44:48.811958Z",
     "shell.execute_reply": "2024-07-09T16:44:48.811040Z",
     "shell.execute_reply.started": "2024-07-09T16:44:48.591036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T16:44:50.784265Z",
     "iopub.status.busy": "2024-07-09T16:44:50.783593Z",
     "iopub.status.idle": "2024-07-09T16:45:00.713893Z",
     "shell.execute_reply": "2024-07-09T16:45:00.713017Z",
     "shell.execute_reply.started": "2024-07-09T16:44:50.784232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a23a8d38ec446e584354f33491377f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1998d357e22d493cbdfdca4ed240f6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/511k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d026dcbb28a3466ab878441795f66c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/156k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55dcc67a26ff4635a97fda680997a49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/94.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65c8d64b8f44052aac6298979738316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cecf6aafb44d21a93c1148cde2a12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cc01b7c8f74e7c9a1ea2a93323c019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'options', 'answers'],\n",
       "        num_rows: 1936\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['context', 'question', 'options', 'answers'],\n",
       "        num_rows: 551\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['context', 'question', 'options', 'answers'],\n",
       "        num_rows: 296\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"Angelectronic/ViMMRC\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T16:45:19.112435Z",
     "iopub.status.busy": "2024-07-09T16:45:19.111781Z",
     "iopub.status.idle": "2024-07-09T16:48:47.737216Z",
     "shell.execute_reply": "2024-07-09T16:48:47.736148Z",
     "shell.execute_reply.started": "2024-07-09T16:45:19.112390Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1033174351e64d60986bf406efcfa605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4320b39786224f30820f2c59c457c6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216a2e72fb63435e8f8f111d70a155a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7beae5d4844c2f863336b5033e7759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5fc35ae640421991abe99f80882321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2983cf10084ba9a6410d1b120f8b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = '',\n",
    ")\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T16:50:33.141312Z",
     "iopub.status.busy": "2024-07-09T16:50:33.140448Z",
     "iopub.status.idle": "2024-07-09T16:50:33.869990Z",
     "shell.execute_reply": "2024-07-09T16:50:33.869072Z",
     "shell.execute_reply.started": "2024-07-09T16:50:33.141278Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbd74174da64f5f9789cd336d91db72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be434f6b5c04be296e8b6841eb0d52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Use the following reference to answer the given multiple choice-question:\n",
      "Reference: Khách đến xã Trịnh Tường, huyện Bát Xát, tỉnh Lào Cai  sẽ không khỏi ngỡ ngàng thấy một dòng mương ngoằn ngoèo vắt ngang những đồi cao. Dân bản gọi dòng mương ấy là con nước ông Lìn. Để thay đổi tập quán làm lúa nương, ông Phàn Phù Lìn, người Dao ở thôn Phìn Ngan đã lần mò cả tháng trong rừng tìm nguồn nước. Nhưng tìm được nguồn nước rồi, mọi người vẫn không tin có thể dẫn nước về. Ông cùng vợ con đào suốt một năm trời được gần bốn cây số mương xuyên đồi dẫn nước từ rừng già về thôn, trồng một héc ta lúa nước để bà con tin. Rồi ông vận động mọi người cùng mở rộng con mương, vỡ thêm đất hoang trồng lúa.\n",
      "\n",
      "Con nước nhỏ đã làm thay đổi tập quán canh tác và cuộc sống của trên 50 hộ trong thôn. Những nương lúa quanh năm khát nước được thay dần bằng ruộng bậc thang. Những giống lúa lai cao sản được ông Lìn đưa về vận động bà con trồng cấy, nhờ vậy mà cả thôn không còn hộ đói. Từ khi nước được dẫn về thôn, nhà ai cũng cấy lúa nước chứ không phá rừng làm nương như trước nữa.\n",
      "\n",
      "Muốn có nước cấy lúa thì phải giữ rừng. Ông Lìn lặn lội đến các xã bạn học cách trồng cây thảo quả về hướng dẫn cho bà con cùng làm. Nhiều hộ trong thôn mỗi năm thu được mấy chục triệu đồng từ loại cây này. Riêng gia đình ông Lìn mỗi năm thu hai trăm triệu. Phìn Ngan từ thôn nghèo nhất đã vươn lên thành thôn có mức sống khá nhất của xã Trịnh Tường.\n",
      "\n",
      "Chuyện của Ngu Công xã Trịnh Tường nhanh chóng bay về Thủ đô. Ông Phàn Phù Lin vinh dự được Chủ tịch nước gửi thư khen ngợi.\n",
      "Question: Gia đình ông Lìn thu được kết quả như thế nào từ việc trồng cây thảo quả?\n",
      "A. Mấy chục triệu.\n",
      "B. Mỗi năm thu hai trăm triệu.\n",
      "C. Mỗi năm thu hai chục triệu.\n",
      "D. Mỗi năm thu hai tỷ.<|eot_id|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "B<|eot_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "END_TURN_TOKEN = '<end_of_turn>'\n",
    "BOS_TOKEN = tokenizer.bos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for i in range(len(examples['question'])):\n",
    "        option_prompt = \"\"\n",
    "        for j in range(len(examples['options'][i])):\n",
    "            if j == 0:\n",
    "                option_prompt += \"A. \" + examples['options'][i][j] + \"\\n\"\n",
    "            elif j == 1:\n",
    "                option_prompt += \"B. \" + examples['options'][i][j] + \"\\n\"\n",
    "            elif j == 2:\n",
    "                option_prompt += \"C. \" + examples['options'][i][j] + \"\\n\"\n",
    "            elif j == 3:\n",
    "                option_prompt += \"D. \" + examples['options'][i][j] + \"\\n\"\n",
    "\n",
    "        chat = [\n",
    "            {\"role\": \"user\", \"content\": \"Use the following reference to answer the given multiple choice-question:\\nReference: \" + examples['context'][i] + \"\\nQuestion: \" + examples['question'][i] + \"\\n\" + option_prompt},\n",
    "            {\"role\": \"system\", \"content\": examples['answers'][i]}\n",
    "        ]\n",
    "        texts.append(tokenizer.apply_chat_template(chat, tokenize=False) + EOS_TOKEN)\n",
    "\n",
    "    texts = [text.split(BOS_TOKEN)[-1] for text in texts]\n",
    "    return {\"text\": texts}\n",
    "pass\n",
    "dataset['train'] = dataset['train'].map(formatting_prompts_func, batched=True)\n",
    "dataset['test'] = dataset['test'].map(formatting_prompts_func, batched=True)\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=\"<start_of_turn>user\", response_template=\"<start_of_turn>model\", tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "print(dataset['train'][-1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "END_TURN_TOKEN = '<|eot_id|>'\n",
    "BOS_TOKEN = tokenizer.bos_token\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "labels = [dataset['test'][i]['answers'] for i in range(len(dataset['test']))]\n",
    "preds = []\n",
    "preds1 = []\n",
    "FastLanguageModel.for_inference(model)\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    option_prompt = \"\"\n",
    "    for j in range(len(dataset['test'][i]['options'])):\n",
    "        if j == 0:\n",
    "            option_prompt += \"A. \" + dataset['test'][i]['options'][j] + \"\\n\"\n",
    "        elif j == 1:\n",
    "            option_prompt += \"B. \" + dataset['test'][i]['options'][j] + \"\\n\"\n",
    "        elif j == 2:\n",
    "            option_prompt += \"C. \" + dataset['test'][i]['options'][j] + \"\\n\"\n",
    "        elif j == 3:\n",
    "            option_prompt += \"D. \" + dataset['test'][i]['options'][j] + \"\\n\"\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Use the following reference to answer the given multiple choice-question, respond only with the letter before the correct answer (A, B, C or D) and do not add anything else :\\nReference: \" + dataset['test'][i]['context'] + \"\\nQuestion: \" + dataset['test'][i]['question'] + \"\\n\" + option_prompt},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    prompt = prompt.split(BOS_TOKEN)[-1]\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256, use_cache = True, do_sample = True, eos_token_id=model.config.eos_token_id, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#     logits = outputs.logits\n",
    "#     token_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    completion = prediction.split('<|start_header_id|>assistant<|end_header_id|>')[-1].split(END_TURN_TOKEN)[0].strip()\n",
    "    preds.append(completion)\n",
    "\n",
    "print(accuracy_score(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
