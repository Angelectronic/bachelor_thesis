{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/lg/.cache/huggingface/datasets/Angelectronic___parquet/Angelectronic--IWSLT15_English_Vietnamese-d3d4a119d05830c0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4a2959ad9941a8a9815d5f9a6d7f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 133166\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "dataset = load_dataset(\"Angelectronic/IWSLT15_English_Vietnamese\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": device}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "embedding = hf.embed_query(\"hi this is harrison\")\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "tm_docs = [Document(page_content=dataset['train'][i]['translation']['en'], metadata={\"id\": dataset['train'][i]['id']}) for i in range(len(dataset['train']))]\n",
    "db = FAISS.from_documents(tm_docs, hf)\n",
    "# db.save_local(\"tm_vectorstore\")\n",
    "# db.similarity_search(\"Nobody 's ever done it before , so I 'm going to go do it .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='He was interviewed once , and he said the following .', metadata={'id': '45660'}),\n",
       " Document(page_content='Asked him what this said .', metadata={'id': '59394'}),\n",
       " Document(page_content=\"Here 's what he had to say .\", metadata={'id': '84341'})]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_db = FAISS.load_local(\"tm_vectorstore\", hf)\n",
    "k = 3\n",
    "retriver = vector_db.as_retriever(search_kwargs={'k': k})\n",
    "relevant_docs = retriver.get_relevant_documents(\"what did he say about ketanji brown jackson\")\n",
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "dataset_copy = copy.deepcopy(dataset)\n",
    "\n",
    "id_list = [data['id'] for data in dataset['train']]\n",
    "rand_id = random.sample(id_list, len(dataset['train'])//2)\n",
    "\n",
    "def get_relevant_docs_train(example):    \n",
    "    if example['id'] in rand_id:\n",
    "        relevant_docs = retriver.get_relevant_documents(example['translation']['en'])\n",
    "        relevant_docs = [doc for doc in relevant_docs if doc.metadata['id'] != example['id']]\n",
    "        if len(relevant_docs) > k - 1:\n",
    "            relevant_docs.pop()\n",
    "\n",
    "        instructs = \"\"              \n",
    "        for doc in relevant_docs:\n",
    "            index = id_list.index(int(doc.metadata['id']))\n",
    "            tm_doc = dataset['train'][index]\n",
    "\n",
    "            instructs += f\"<English> : {tm_doc['translation']['en']}\\n<Vietnamese> : {tm_doc['translation']['vi']}\\n\\n\"\n",
    "            \n",
    "        example['translation']['en'] = instructs + \"<English> : \" + example['translation']['en'] + \"\\n\" + \"<Vietnamese> : \"\n",
    "    else:\n",
    "        example['translation']['en'] = \"<English> : \" + example['translation']['en'] + \"\\n\" + \"<Vietnamese> : \"\n",
    "    \n",
    "    return example\n",
    "\n",
    "def get_relevant_docs_test(example):    \n",
    "    relevant_docs = retriver.get_relevant_documents(example['translation']['en'])\n",
    "    relevant_docs = [doc for doc in relevant_docs if doc.metadata['id'] != example['id']]\n",
    "    if len(relevant_docs) > k - 1:\n",
    "        relevant_docs.pop()\n",
    "\n",
    "    instructs = \"\"              \n",
    "    for doc in relevant_docs:\n",
    "        index = id_list.index(int(doc.metadata['id']))\n",
    "        tm_doc = dataset['train'][index]\n",
    "\n",
    "        instructs += f\"<English> : {tm_doc['translation']['en']}\\n<Vietnamese> : {tm_doc['translation']['vi']}\\n\\n\"\n",
    "        \n",
    "    example['translation']['en'] = instructs + \"<English> : \" + example['translation']['en'] + \"\\n\" + \"<Vietnamese> : \"\n",
    "    return example\n",
    "\n",
    "dataset_copy['train'] = dataset_copy['train'].map(get_relevant_docs_train)\n",
    "dataset_copy['test'] = dataset_copy['test'].map(get_relevant_docs_test)\n",
    "# dataset_copy.save_to_disk(\"fuzzy_iwslt15\")\n",
    "\n",
    "print(dataset_copy['train'][0]['translation']['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b93f8aef5ff436aaa59fad98bf03c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to C:/Users/lg/.cache/huggingface/datasets/Angelectronic___parquet/Angelectronic--fuzzy_iwslt15_domain_specific-918d73d5c01a1810/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333c371bd948446fbf44df3931bcb62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d874ec72254edc85f95ff1ff3b8c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/48.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02f3444194b4987865830c435261046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/738k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71a029ca37549e2a86b7175c09850ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42dc586d8a747bd9072a98597167480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1599b78b5b74f469aaa5076dea06343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/131621 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444116802b0e42fabfa2cde7f772d99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cfb9ef42b44736b5303a99cf0d2ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating domain_specific_test split:   0%|          | 0/1542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/lg/.cache/huggingface/datasets/Angelectronic___parquet/Angelectronic--fuzzy_iwslt15_domain_specific-918d73d5c01a1810/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93562a246f84d22a8dbdc70435242f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 131621\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "    domain_specific_test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 1542\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "sample_size_train = 133166\n",
    "sample_size_test = 1268\n",
    "dataset = load_dataset(\"Angelectronic/fuzzy_iwslt15_domain_specific\")\n",
    "\n",
    "# random_indices_train = random.sample(range(len(dataset['train'])), sample_size_train)\n",
    "# dataset['train'] = dataset['train'].select(random_indices_train)\n",
    "\n",
    "# random_indices_test = random.sample(range(len(dataset['test'])), sample_size_test)\n",
    "# dataset['test'] = dataset['test'].select(random_indices_test)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = 'Float16' # None for auto detection. torch.float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = userdata.get('HF_TOKEN'), # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    translations = examples['translation']\n",
    "    for i in range(len(translations)):\n",
    "        text = translations[i]['en'] + translations[i]['vi'] + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "pass\n",
    "dataset['train'] = dataset['train'].map(formatting_prompts_func, batched=True)\n",
    "print(dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "class EvaluateAfterEpochCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, model, tokenizer, **kwargs):\n",
    "        EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "        labels = [doc['translation']['vi'] + EOS_TOKEN for doc in dataset['test']]\n",
    "        predictions = []\n",
    "        for test_example in dataset['test']:\n",
    "            prompt = test_example['translation']['en']\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors = \"pt\",\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "            prediction = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "\n",
    "            completion = prediction.split(prompt)[-1].split(EOS_TOKEN)[0].strip()\n",
    "            predictions.append(completion)\n",
    "\n",
    "        results = metric.compute(predictions=predictions, references=labels)\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset['train'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    callbacks = [EvaluateAfterEpochCallback()],\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        report_to=\"tensorboard\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Gemma-7b_en_vi\",\n",
    "        push_to_hub=True,\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "preds = [\"Xin chào mọi người!\", \"Hoàng hôn đẹp quá!\", \"Tôi là một người đàn ông!\"]\n",
    "labels = [\"Xin chào thế giới!\", \"Hoàng hôn không đẹp!\", \"Tôi là một người phụ nữ!\"]\n",
    "metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Angelectronic/Gemma-7b_en_vi\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "BOS_TOKEN = tokenizer.bos_token\n",
    "\n",
    "labels = [doc['translation']['vi'] + EOS_TOKEN for doc in dataset['test']]\n",
    "predictions = []\n",
    "for test_example in dataset['test']:\n",
    "    prompt = test_example['translation']['en']\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "\n",
    "    completion = prediction.split(prompt)[-1].split(EOS_TOKEN)[0].strip()\n",
    "    predictions.append(completion)\n",
    "\n",
    "results = metric.compute(predictions=predictions, references=labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
