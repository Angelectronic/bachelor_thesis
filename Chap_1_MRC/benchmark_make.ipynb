{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/lg/.cache/huggingface/datasets/Angelectronic___parquet/Angelectronic--IWSLT15_English_Vietnamese-d3d4a119d05830c0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baefd77aed5246069c86074b113b7b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 133166\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "sample_size_train = 133166\n",
    "sample_size_test = 1268\n",
    "dataset = load_dataset(\"Angelectronic/IWSLT15_English_Vietnamese\")\n",
    "random_indices_train = random.sample(range(len(dataset['train'])), sample_size_train)\n",
    "dataset['train'] = dataset['train'].select(random_indices_train)\n",
    "\n",
    "random_indices_test = random.sample(range(len(dataset['test'])), sample_size_test)\n",
    "dataset['test'] = dataset['test'].select(random_indices_test)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"https://skier8402-mistral-super-fast.hf.space/\")\n",
    "\n",
    "def domain_specific(text):\n",
    "    global client\n",
    "    prompt_template = '''Does this sentence contain domain-specific words? Return 1 if it does, 0 otherwise. Don't elaborate any further. Sentence: \"{}\"'''\n",
    "\n",
    "    result = client.predict(\n",
    "\t\t\t\tprompt_template.format(text),\n",
    "\t\t\t\t0.5,\t# int | float (numeric value between 0.0 and 1.0) in 'Temperature' Slider component\n",
    "\t\t\t\t128,\t# int | float (numeric value between 0 and 1048) in 'Max new tokens' Slider component\n",
    "\t\t\t\t0.9,\t# int | float (numeric value between 0.0 and 1) in 'Top-p (nucleus sampling)' Slider component\n",
    "\t\t\t\t1.2,\t# int | float (numeric value between 1.0 and 2.0) in 'Repetition penalty' Slider component\n",
    "\t\t\t\tapi_name=\"/chat\"\n",
    "    )\n",
    "\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_data_to_disk(data):\n",
    "    with open('iwslt15_en_vi_domain_specific.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "data_train = []\n",
    "with open('iwslt/iwslt15_en_vi_domain_specific.json', 'r', encoding='utf-8') as f:\n",
    "    data_train = json.load(f)        \n",
    "\n",
    "save_step = 200\n",
    "for i in tqdm(range(len(dataset['train']))):\n",
    "    data = dict(dataset['train'][i])\n",
    "\n",
    "    if i % save_step == 0:\n",
    "        save_data_to_disk(data_train)\n",
    "        print(f\"Processing {i}th example\")\n",
    "\n",
    "    if data['id'] in [d['id'] for d in data_train]:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        resp = domain_specific(dataset['train'][i]['translation']['en'])\n",
    "        if resp[0] == '1' or resp[:3] == 'Yes':\n",
    "            data['domain_specific'] = True\n",
    "        elif resp[0] == '0' or resp[:2] == 'No':\n",
    "            data['domain_specific'] = False\n",
    "        else:\n",
    "            data['domain_specific'] = None\n",
    "            data['domain_specific_response'] = resp\n",
    "    except:\n",
    "        save_data_to_disk(data_train)\n",
    "        client = Client(\"https://skier8402-mistral-super-fast.hf.space/\")\n",
    "\n",
    "        resp = domain_specific(dataset['train'][i]['translation']['en'])\n",
    "        if resp[0] == '1' or resp[:3] == 'Yes':\n",
    "            data['domain_specific'] = True\n",
    "        elif resp[0] == '0' or resp[:2] == 'No':\n",
    "            data['domain_specific'] = False\n",
    "        else:\n",
    "            data['domain_specific'] = None\n",
    "            data['domain_specific_response'] = resp\n",
    "    data_train.append(data)\n",
    "\n",
    "save_data_to_disk(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4718 1572 1573 1573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\lg\\.cache\\huggingface\\datasets\\Angelectronic___parquet\\Angelectronic--IWSLT15_English_Vietnamese-d3d4a119d05830c0\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-8198e0d7d9067204.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81e399293e545b8a88fe1b3fbaad389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/131621 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 131621\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 1268\n",
      "    })\n",
      "    domain_specific_test: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 1542\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "\n",
    "with open('iwslt/iwslt15_en_vi_domain_specific_1.json', 'r', encoding='utf-8') as f:\n",
    "    data_1 = json.load(f)\n",
    "\n",
    "with open('iwslt/iwslt15_en_vi_domain_specific_2.json', 'r', encoding='utf-8') as f:\n",
    "    data_2 = json.load(f)\n",
    "\n",
    "with open('iwslt/iwslt15_en_vi_domain_specific_3.json', 'r', encoding='utf-8') as f:\n",
    "    data_3 = json.load(f)\n",
    "\n",
    "data_123 = data_1 + data_2 + data_3\n",
    "print(len(data_123), len(data_1), len(data_2), len(data_3))\n",
    " \n",
    "id_list = [d['id'] for d in data_123]\n",
    "true_ids = [d['id'] for d in data_123 if d['domain_specific'] == True]\n",
    "null_ids_true = [d['id'] for d in data_123 if d['domain_specific'] == None and 'translation' in d]\n",
    "null_ids_false = [d['id'] for d in data_123 if d['domain_specific'] == None and 'translation' not in d]\n",
    "\n",
    "test_dm_dict = dict()\n",
    "test_dm_dict['id'] = []\n",
    "test_dm_dict['translation'] = []\n",
    "for d in dataset['train']:\n",
    "    if d['id'] in true_ids:\n",
    "        test_dm_dict['id'].append(d['id'])\n",
    "        test_dm_dict['translation'].append(d['translation'])\n",
    "\n",
    "def delete_example(example):\n",
    "    if example['id'] in null_ids_false or example['id'] in true_ids:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def change_translation(example):\n",
    "    if example['id'] in null_ids_true:\n",
    "        index = id_list.index(example['id'])\n",
    "        example['translation'] = data_123[index]['translation']\n",
    "    return example\n",
    "\n",
    "dataset['train'] = dataset['train'].filter(delete_example).map(change_translation)\n",
    "\n",
    "new_dataset = DatasetDict({'train': dataset['train'], 'test': dataset['test'], 'domain_specific_test': Dataset.from_dict(test_dm_dict)})\n",
    "print(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": device}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "embedding = hf.embed_query(\"hi this is harrison\")\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Nobody 's ever done it before , so I 'm going to go do it .\", metadata={'id': 29184}),\n",
       " Document(page_content=\"I just did something I 've never done before .\", metadata={'id': 43182}),\n",
       " Document(page_content='I did something that nobody else had done .', metadata={'id': 109867}),\n",
       " Document(page_content=\"So , talk about risk taking . I 'm going to do somebody that nobody likes .\", metadata={'id': 51443})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "tm_docs = [Document(page_content=new_dataset['train'][i]['translation']['en'], metadata={\"id\": new_dataset['train'][i]['id']}) for i in range(len(new_dataset['train']))]\n",
    "db = FAISS.from_documents(tm_docs, hf)\n",
    "db.save_local(\"tm_vectorstore\")\n",
    "db.similarity_search(\"Nobody 's ever done it before , so I 'm going to go do it .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='He was interviewed once , and he said the following .', metadata={'id': 45660}),\n",
       " Document(page_content='Asked him what this said .', metadata={'id': 59394}),\n",
       " Document(page_content=\"Here 's what he had to say .\", metadata={'id': 84341})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_db = FAISS.load_local(\"tm_vectorstore\", hf)\n",
    "k = 3\n",
    "retriver = vector_db.as_retriever(search_kwargs={'k': k})\n",
    "relevant_docs = retriver.get_relevant_documents(\"what did he say about ketanji brown jackson\")\n",
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee1549a2e5b458da180879c75e03b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/131621 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f329736eba4746279f31f92610e78b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3998aa5c5b4cb49db881e1f82a7bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413ba3cd77054bd89c4a159fa0262429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e6b24ea5774730b4ff3892dd2d43af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/132 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce908e316a904733acf380d4922c817a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0272bde28458419e9a3295bfd1e0e01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split domain_specific_test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e93e747b94c4561bbb8eabc20d6cf91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b99affbaf849ce90ce9eda7c413b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<English> : I know how to be Mowgli .\n",
      "<Vietnamese> : \n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "dataset_copy = copy.deepcopy(new_dataset)\n",
    "\n",
    "id_list = [data['id'] for data in new_dataset['train']]\n",
    "rand_id = random.sample(id_list, len(new_dataset['train'])//2)\n",
    "\n",
    "def get_relevant_docs_train(example):    \n",
    "    if example['id'] in rand_id:\n",
    "        relevant_docs = retriver.get_relevant_documents(example['translation']['en'])\n",
    "        relevant_docs = [doc for doc in relevant_docs if doc.metadata['id'] != example['id']]\n",
    "        if len(relevant_docs) > k - 1:\n",
    "            relevant_docs.pop()\n",
    "\n",
    "        instructs = \"\"              \n",
    "        for doc in relevant_docs:\n",
    "            index = id_list.index(int(doc.metadata['id']))\n",
    "            tm_doc = new_dataset['train'][index]\n",
    "\n",
    "            instructs += f\"<English> : {tm_doc['translation']['en']}\\n<Vietnamese> : {tm_doc['translation']['vi']}\\n\\n\"\n",
    "            \n",
    "        example['translation']['en'] = instructs + \"<English> : \" + example['translation']['en'] + \"\\n\" + \"<Vietnamese> : \"\n",
    "    else:\n",
    "        example['translation']['en'] = \"<English> : \" + example['translation']['en'] + \"\\n\" + \"<Vietnamese> : \"\n",
    "    \n",
    "    return example\n",
    "\n",
    "def get_relevant_docs_test(example):    \n",
    "    relevant_docs = retriver.get_relevant_documents(example['translation']['en'])\n",
    "    relevant_docs = [doc for doc in relevant_docs if doc.metadata['id'] != example['id']]\n",
    "    if len(relevant_docs) > k - 1:\n",
    "        relevant_docs.pop()\n",
    "\n",
    "    instructs = \"\"              \n",
    "    for doc in relevant_docs:\n",
    "        index = id_list.index(int(doc.metadata['id']))\n",
    "        tm_doc = new_dataset['train'][index]\n",
    "\n",
    "        instructs += f\"<English> : {tm_doc['translation']['en']}\\n<Vietnamese> : {tm_doc['translation']['vi']}\\n\\n\"\n",
    "        \n",
    "    example['translation']['en'] = instructs + \"<English> : \" + example['translation']['en'] + \"\\n\" + \"<Vietnamese> : \"\n",
    "    return example\n",
    "\n",
    "dataset_copy['train'] = dataset_copy['train'].map(get_relevant_docs_train)\n",
    "dataset_copy['test'] = dataset_copy['test'].map(get_relevant_docs_test)\n",
    "dataset_copy['domain_specific_test'] = dataset_copy['domain_specific_test'].map(get_relevant_docs_test)\n",
    "# dataset_copy.push_to_hub(\"Angelectronic/fuzzy_iwslt15_domain_specific\")\n",
    "\n",
    "print(dataset_copy['train'][0]['translation']['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
