# -*- coding: utf-8 -*-
"""gemma2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IbMCUcGd2_og1xngApne_095h41LIVo1
"""

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import evaluate
from transformers.trainer_callback import TrainerCallback
from tqdm import tqdm
from trl import SFTTrainer
from transformers import TrainingArguments
import re
import evaluate
import numpy as np
import torch
from peft import LoraConfig, PeftConfig
import os
import random
from huggingface_hub import login
os.environ['HF_HOME'] = '/workspace/std_nlp_1/cache'
os.environ['HF_DATASETS_CACHE'] = '/workspace/std_nlp_1/cache/datasets'
os.environ['HF_HUB_CACHE'] = '/workspace/std_nlp_1/cache/hub'
random.seed(42)
login('')

# get dataset
sample_size_train = 133316
sample_size_test = 1268

dataset = load_dataset("Angelectronic/fuzzy_iwslt15_domain_specific")
model_name = "google/gemma-7b"
print(dataset)


# Set up the quantization model
compute_dtype = getattr(torch, "float16")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=False,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
)

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0,
    r=16,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
)

model.config.use_cache = False
model.config.pretraining_tp = 1

max_seq_length = 256
tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)
tokenizer.padding_side = 'right'

# set up the dataset for training
EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    texts = []
    translations = examples['translation']
    for i in range(len(translations)):
        text = translations[i]['en'] + translations[i]['vi'] + EOS_TOKEN
        texts.append(text)

    return {"text": texts}
pass
dataset['train'] = dataset['train'].map(formatting_prompts_func, batched=True)
dataset['test'] = dataset['test'].map(formatting_prompts_func, batched=True)
print(dataset['train'][0]['text'])

# set up the callback for evaluation
metric = evaluate.load("sacrebleu")

class EvaluateAfterEpochCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, model, tokenizer, **kwargs):
        EOS_TOKEN = tokenizer.eos_token
        labels = [doc['translation']['vi'] + EOS_TOKEN for doc in dataset['test']]
        predictions = []
        for test_example in tqdm(dataset['test']):
            prompt = test_example['translation']['en']

            inputs = tokenizer(
                prompt,
                return_tensors = "pt",
            ).to("cuda")

            outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
            prediction = tokenizer.decode(outputs[0], skip_special_tokens = True)

            completion = prediction.split(prompt)[-1].split(EOS_TOKEN)[0].strip()
            predictions.append(completion)

        results = metric.compute(predictions=predictions, references=labels)
        print(results)


# set up the trainer
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    peft_config=peft_config,
    train_dataset = dataset['train'],
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    callbacks = [EvaluateAfterEpochCallback()],
    args = TrainingArguments(
        # per_device_train_batch_size = 2,
        auto_find_batch_size=True,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        num_train_epochs=1,
        learning_rate = 2e-4,
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 200,
        optim = "adamw_8bit",
        report_to="tensorboard",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "Gemma-7b_en_vi",
        push_to_hub=True,
        # evaluation_strategy='steps'
    ),
)
trainer_stats = trainer.train()